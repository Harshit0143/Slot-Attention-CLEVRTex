{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cf2f47b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-07T10:08:54.727717Z",
     "iopub.status.busy": "2024-05-07T10:08:54.727371Z",
     "iopub.status.idle": "2024-05-07T10:09:00.458528Z",
     "shell.execute_reply": "2024-05-07T10:09:00.457453Z"
    },
    "papermill": {
     "duration": 5.738606,
     "end_time": "2024-05-07T10:09:00.460783",
     "exception": false,
     "start_time": "2024-05-07T10:08:54.722177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's Go!\n",
      "torch version: 2.1.2\n"
     ]
    }
   ],
   "source": [
    "print(\"Let's Go!\" )\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "import random \n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LayerNorm\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn import BatchNorm2d\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import Linear\n",
    "from torch.nn import DataParallel\n",
    "from torch.nn import Sequential\n",
    "from torch.nn import ConvTranspose2d\n",
    "from torch.nn import GRUCell\n",
    "from torch.nn import Module\n",
    "from torch.nn import Flatten\n",
    "from torch.nn.init import normal_\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import softmax\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Conv1d\n",
    "from torch.nn import MSELoss\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import h5py\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "print(\"torch version:\" , torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "348baec0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T10:09:00.471103Z",
     "iopub.status.busy": "2024-05-07T10:09:00.470136Z",
     "iopub.status.idle": "2024-05-07T10:10:02.845608Z",
     "shell.execute_reply": "2024-05-07T10:10:02.844433Z"
    },
    "papermill": {
     "duration": 62.383612,
     "end_time": "2024-05-07T10:10:02.848391",
     "exception": false,
     "start_time": "2024-05-07T10:09:00.464779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices shape: (10000,)\n",
      "clusters data shape: (10000, 128, 128)\n",
      "Loading data!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:52<00:00, 191.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data!\n",
      "data size: 10000\n",
      "image tensor max-min 0.9686274528503418 -1.0\n",
      "cluster tensor max-min 9 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images_val = '/kaggle/input/clevertexv11-val-images/CLEVERTex10_val_images'\n",
    "clever6_indices_path = '/kaggle/input/clevertex6-val/CLEVERTex6_val/CLEVRTex6_val_indices.npy'\n",
    "val_cluster_path = '/kaggle/input/clevertex6-val/CLEVERTex6_val/val_masks_clustering.npy'\n",
    "clever6_indices = np.load(clever6_indices_path)\n",
    "clever11_indices = np.array([i + 40000 for i in range(10000)])\n",
    "\n",
    "class ImageDatasetSlotAttention(Dataset):\n",
    "    def __init__(self, images_folder , indices , transform , cluster_path):\n",
    "        super(ImageDatasetSlotAttention , self).__init__()\n",
    "        self.images_folder = images_folder\n",
    "        self.folder = images_folder\n",
    "        self.transform = transform\n",
    "        self.indices = indices\n",
    "        self.all_clusters = np.load(cluster_path)\n",
    "        print(\"indices shape:\" , self.indices.shape)\n",
    "        print(\"clusters data shape:\" , self.all_clusters.shape)\n",
    "        self.all_data =  self.build_data(images_folder)\n",
    "        print(\"data size:\" , len(self.all_data))\n",
    "\n",
    "        del self.all_clusters\n",
    "        del self.transform \n",
    "\n",
    "    def build_data(self , folder_path):\n",
    "        files = sorted(os.listdir(folder_path))\n",
    "        files = sorted(files)\n",
    "        data = []\n",
    "        print(\"Loading data!\")\n",
    "        for idx in tqdm(self.indices):\n",
    "            \n",
    "            image_path = self.images_folder + f'/CLEVRTEX_full_{idx:06d}.png'\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            cluster = torch.tensor(self.all_clusters[idx - 40000])\n",
    "            data.append((self.transform(image) , cluster))\n",
    "            \n",
    "        print(\"Loaded data!\")\n",
    "        return data\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self , idx):\n",
    "        return self.all_data[idx]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "  transforms.PILToTensor(),\n",
    "  transforms.ConvertImageDtype(torch.float),\n",
    "  transforms.Normalize((0.5 ,  0.5 , 0.5), (0.5, 0.5 , 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "val_data = ImageDatasetSlotAttention(images_folder = images_val , indices = clever11_indices,\n",
    "                                    transform = transform, cluster_path = val_cluster_path)\n",
    "print(\"image tensor max-min\" , val_data[0][0].max().item() , val_data[0][0].min().item())\n",
    "print(\"cluster tensor max-min\" , val_data[4][1].max().item() , val_data[4][1].min().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02502e3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T10:10:02.934004Z",
     "iopub.status.busy": "2024-05-07T10:10:02.933146Z",
     "iopub.status.idle": "2024-05-07T10:10:02.947820Z",
     "shell.execute_reply": "2024-05-07T10:10:02.946892Z"
    },
    "papermill": {
     "duration": 0.058695,
     "end_time": "2024-05-07T10:10:02.949948",
     "exception": false,
     "start_time": "2024-05-07T10:10:02.891253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(Module):\n",
    "    def __init__(self, channels):\n",
    "        super(Block, self).__init__()\n",
    "        self.downsample = Sequential(OrderedDict([\n",
    "            ('conv1' , Conv2d(channels, channels, kernel_size = 5 , stride = 1 , padding = 2)),\n",
    "            ('bn1'   , BatchNorm2d(channels)),\n",
    "            ('relu1' , ReLU()),\n",
    "            ('conv2' , Conv2d(channels, channels, kernel_size = 5 , stride = 1 , padding = 2)),\n",
    "            ('bn2'   , BatchNorm2d(channels))\n",
    "        ]))\n",
    "        \n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + self.downsample(x))\n",
    "\n",
    "class BlockUp(Module):\n",
    "    def __init__(self , in_channels , out_channels):\n",
    "        super(BlockUp, self).__init__()\n",
    "        \n",
    "        self.downsample = Sequential(OrderedDict([\n",
    "            ('conv1' , Conv2d(in_channels , out_channels , kernel_size = 5 , stride = 1 , padding = 2)),\n",
    "            ('bn1'   , BatchNorm2d(out_channels)),\n",
    "            ('relu1' , ReLU()),\n",
    "            ('conv2' , Conv2d(out_channels , out_channels , kernel_size = 5 , stride = 1 , padding = 2)),\n",
    "            ('bn2'   , BatchNorm2d(out_channels)),\n",
    "        ]))\n",
    "        self.skip = Sequential(OrderedDict([\n",
    "            ('conv1' , Conv2d(in_channels , out_channels, kernel_size = 5 , stride = 1 , padding = 2)),\n",
    "            ('bn1'   , BatchNorm2d(out_channels))\n",
    "        ]))\n",
    "        self.relu = ReLU()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.skip(x) + self.downsample(x))\n",
    "    \n",
    "    \n",
    "class ResNet18(Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(ResNet18, self).__init__()\n",
    "        layer0 = Sequential(OrderedDict([\n",
    "         ('conv' , Conv2d(3 , 16 , kernel_size = 5 , stride = 1 , padding = 2)),\n",
    "         ('bn'   , BatchNorm2d(16)),\n",
    "         ('relu' , ReLU())\n",
    "        ]))\n",
    "\n",
    "        self.resnet = Sequential(OrderedDict([\n",
    "            ('layer0' , layer0),\n",
    "            ('block1' , Block(16)),\n",
    "            ('block2' , Block(16)),\n",
    "            ('block3' , BlockUp(16 , 32)),\n",
    "            ('block4' , Block(32)),\n",
    "            ('block5' , Block(32)),\n",
    "            ('block6' , BlockUp(32 , 64)),\n",
    "            ('block7' , Block(64)),\n",
    "            ('block8' , Block(64))\n",
    "        ]))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25af9449",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T10:10:03.038481Z",
     "iopub.status.busy": "2024-05-07T10:10:03.038144Z",
     "iopub.status.idle": "2024-05-07T10:10:03.074200Z",
     "shell.execute_reply": "2024-05-07T10:10:03.073291Z"
    },
    "papermill": {
     "duration": 0.082456,
     "end_time": "2024-05-07T10:10:03.076098",
     "exception": false,
     "start_time": "2024-05-07T10:10:02.993642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ObjectDiscovery(Module):\n",
    "    def __init__(self , encoder_resolution ,  decoder_resolution , T   , K , D_slots):\n",
    "        super(ObjectDiscovery , self).__init__()\n",
    "        print(f'Initialized ObjectDiscovery!')\n",
    "        self.layers = Sequential(OrderedDict([\n",
    "          (\"encoder\" , ImageEncoder(resolution = encoder_resolution , T = T,\n",
    "                                    K = K , D_slots = D_slots )),\n",
    "          (\"decoder\" , SlotAttentionDecoder(resolution = decoder_resolution , K = K , D_slots = D_slots))\n",
    "        ]))\n",
    "    def forward(self , image):\n",
    "        return self.layers(image)\n",
    "\n",
    "class PositionEncoder(Module):\n",
    "    def __init__(self, output_dim , resolution):\n",
    "        super(PositionEncoder , self).__init__()\n",
    "        self.linear =  Linear(in_features = 4 , out_features = output_dim)\n",
    "        # above is equivalent to a linear layer\n",
    "        self.grid = Parameter(data = PositionEncoder.build_grid(resolution) , requires_grad = False)\n",
    "        print(\"Grid shape:\" , self.grid.shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_grid(resolution):\n",
    "        ranges = [np.linspace(start = 0.0 , stop = 1.0 , num = dimension) for dimension in resolution] # dim = (2 , 128)\n",
    "        grid = np.meshgrid(*ranges , sparse = False, indexing = \"ij\") # dim = (128 , 128)\n",
    "        # row[i] of grid[0] has all elements i / 127 and col[j] of grid[1] has all elements j / 127\n",
    "        grid = np.stack(grid , axis = -1) # dim = (64 , 64 , 2) to match conv later\n",
    "        grid = np.expand_dims(grid, axis = 0) # dim = (1 , 64 , 64 , 2) for batch dimension\n",
    "        grid = grid.astype(np.float32) # PyTorch throws an error later otherwise\n",
    "        return torch.tensor(np.concatenate([grid , 1.0 - grid] , axis = 3)) # (1 , 64 , 64 , 4)\n",
    "\n",
    "\n",
    "    def forward(self, x): # x has shape (batch , 64 , 64 , D_inputs)\n",
    "        return x + self.linear(self.grid)\n",
    "\n",
    "\n",
    "class ImageEncoder(Module):\n",
    "    def __init__(self , resolution , T  ,  K , D_slots):\n",
    "        super(ImageEncoder , self).__init__()\n",
    "        print(f'Initialized ImageEncoder! resolution: {resolution}')\n",
    "        D_inputs = 64\n",
    "        self.encoder_cnn = ResNet18()\n",
    "        down_resolution = (128 , 128)\n",
    "        positional_encoder = PositionEncoder(output_dim =  D_inputs , resolution = down_resolution)\n",
    "\n",
    "\n",
    "        slot_attention = SlotAttention(T = T , K = K , D_slots = D_slots)\n",
    "        self.pos_encode_feedforward_slotattn = Sequential(OrderedDict([\n",
    "            (\"pos_encoder\" , positional_encoder), #  (batch , 64 , 64 , D_inputs)\n",
    "            (\"flatten\" , Flatten(start_dim = 1 , end_dim = 2)), # (batch , 64 * 64 , D_inputs)\n",
    "            (\"layer_norm\" ,  LayerNorm(normalized_shape = 64)), # (batch , 64 * 64 , D_inputs)\n",
    "            (\"linear1:\" , Linear(in_features = 64 , out_features = 128)),\n",
    "            (\"relu1:\"   , ReLU()),\n",
    "            (\"linear2:\" , Linear(in_features = 128 , out_features = 128)),\n",
    "            (\"slot_attention\" , slot_attention) #  (batch , K , D_slots)\n",
    "        ]))\n",
    "    # feature map has shape (channels , h , w)\n",
    "    # N is h * w i.e. each pixel is a different feature \"vector\", size of this vector of D_inputs = # of channels\n",
    "    # channels = D_inputs , h * w = N\n",
    "    def forward(self , x): # x is aimge with shape (batch , 3 , 126 , 128)\n",
    "        x = self.encoder_cnn(x).permute(dims = (0 , 2 , 3 , 1))  # (batch , 64 , 64 , D_inputs)\n",
    "        return self.pos_encode_feedforward_slotattn(x)\n",
    "\n",
    "\n",
    "\n",
    "class SlotAttention(Module):\n",
    "    def __init__(self , T, K , D_slots  , epsilon = 1e-8):\n",
    "        super(SlotAttention , self).__init__()\n",
    "        print(f'Initialzing SlotAttention parameters: T: {T} , K: {K} , D_slots: {D_slots}')\n",
    "        self.T = T\n",
    "        self.K = K\n",
    "        self.D_slots = D_slots\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.norm_inputs = LayerNorm(normalized_shape =  128)\n",
    "        self.query_from_slots  = Sequential(OrderedDict([\n",
    "            (\"NormSlots\" , LayerNorm(normalized_shape = D_slots)),\n",
    "            (\"SlotQuery\" , Linear(in_features = D_slots  , out_features = D_slots , bias = False))\n",
    "        ]))\n",
    "\n",
    "\n",
    "#         self.slots_init = Parameter(data = normal_(torch.empty(1 , K , D_slots)) , requires_grad = True)\n",
    "\n",
    "        self.init_latents = Parameter(normal_(torch.empty(1 , self.K , self.D_slots)))\n",
    "\n",
    "\n",
    "\n",
    "        self.keys_from_inputs  = Linear(in_features = 128 , out_features = D_slots , bias = False)\n",
    "        self.vals_from_inputs  = Linear(in_features = 128 , out_features = D_slots , bias = False)\n",
    "        self.gru = GRUCell(input_size = D_slots , hidden_size = D_slots)\n",
    "        self.norm_feed = Sequential(OrderedDict([\n",
    "            (\"layer_norm\" ,  LayerNorm(normalized_shape = D_slots)),\n",
    "            (\"linear1\" , Linear(in_features = D_slots , out_features = 256)),\n",
    "            (\"relu1\"   , ReLU()),\n",
    "            (\"linear2\" , Linear(in_features = 256 , out_features = D_slots))\n",
    "        ]))\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.shape[0]\n",
    "        inputs = self.norm_inputs(inputs)\n",
    "        keys = self.keys_from_inputs(inputs)\n",
    "        vals = self.vals_from_inputs(inputs)\n",
    "        slots = self.init_latents.repeat(batch_size , 1 , 1)\n",
    "        for _ in range(self.T):\n",
    "            slots_prev = slots\n",
    "            queries = self.query_from_slots(slots)\n",
    "            attn_logits = (self.D_slots ** -0.5) * torch.einsum('bnc,bmc->bnm', keys , queries)\n",
    "            attn = F.softmax(attn_logits , dim = -1)\n",
    "            attn = attn + self.epsilon\n",
    "            attn = attn / torch.sum(attn, dim = 1, keepdim = True)\n",
    "            updates = torch.einsum('bnm,bnc->bmc' , attn , vals)\n",
    "            slots = self.gru(\n",
    "                updates.view(batch_size * self.K , self.D_slots),\n",
    "                slots_prev.view(batch_size * self.K , self.D_slots),\n",
    "            )\n",
    "            slots = slots.view(batch_size , self.K , self.D_slots)\n",
    "            slots = slots + self.norm_feed(slots)\n",
    "\n",
    "        return slots\n",
    "\n",
    "\n",
    "class SlotAttentionDecoder(Module):\n",
    "    def __init__(self , resolution , K , D_slots):\n",
    "        super(SlotAttentionDecoder , self).__init__()\n",
    "        print(f'Initialized SlotAttentionDecoder! resolution: {resolution}')\n",
    "        print(f'Decoder parameters: K: {K} , D_slots: {D_slots}')\n",
    "        self.resolution = resolution\n",
    "        self.positional_encoder = PositionEncoder(output_dim = D_slots , resolution = resolution)\n",
    "\n",
    "        self.K = K\n",
    "        self.D_slots = D_slots\n",
    "\n",
    "        self.decoder_cnn = Sequential(OrderedDict([\n",
    "            (\"conv1\" , ConvTranspose2d(in_channels = D_slots , out_channels = 64 , kernel_size = 5 , stride = 2 , padding = 2 , output_padding = 1)),\n",
    "            ('relu1' , ReLU()),\n",
    "            (\"conv2\" , ConvTranspose2d(in_channels = 64 , out_channels = 64 , kernel_size = 5 , stride = 2 , padding = 2 , output_padding = 1)),\n",
    "            ('relu2' , ReLU()),\n",
    "            (\"conv3\" , ConvTranspose2d(in_channels = 64 , out_channels = 64 , kernel_size = 5 , stride = 2 , padding = 2 , output_padding = 1)),\n",
    "            ('relu3' , ReLU()),\n",
    "            (\"conv4\" , ConvTranspose2d(in_channels = 64 , out_channels = 64 , kernel_size = 5 , stride = 2 , padding = 2 , output_padding = 1)),\n",
    "            ('relu4' , ReLU()),\n",
    "            (\"conv5\" , ConvTranspose2d(in_channels = 64 , out_channels = 4 , kernel_size = 1 , stride = 1)),\n",
    "        ]))\n",
    "\n",
    "\n",
    "    def forward(self , slots):\n",
    "        # slots comes form the ImageEncoder shape : (batch , K , D_slots)\n",
    "        batch_size = slots.shape[0]\n",
    "        x =  slots.view(batch_size * self.K , 1 , 1 , self.D_slots)\n",
    "        x = x.repeat(1 , self.resolution[0] , self.resolution[1] , 1)\n",
    "        x = self.positional_encoder(x).permute(0 , 3 , 1 , 2)\n",
    "        x = self.decoder_cnn(x) # dim = (batch * K , 4 , 128 , 128)\n",
    "        x = x.view(batch_size, self.K , 4 , 128 , 128)# dim = (batch , K , 4 , 128 , 128)\n",
    "        recons , masks = x[: , : , : 3 , : , :]  , x[: , : , -1 : , : , :]\n",
    "        masks = F.softmax(masks , dim = 1) # dim = (batch , K , 1 , 128, 128)\n",
    "        reconstructed = torch.sum(recons * masks , dim = 1) # dim = (batch , 3 , 128, 128)\n",
    "        return reconstructed , recons , masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1fc9db3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T10:10:03.163904Z",
     "iopub.status.busy": "2024-05-07T10:10:03.163567Z",
     "iopub.status.idle": "2024-05-07T10:10:03.694269Z",
     "shell.execute_reply": "2024-05-07T10:10:03.693340Z"
    },
    "papermill": {
     "duration": 0.577445,
     "end_time": "2024-05-07T10:10:03.696372",
     "exception": false,
     "start_time": "2024-05-07T10:10:03.118927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model!\n",
      "Using device: cuda\n",
      "Initialized ObjectDiscovery!\n",
      "Initialized ImageEncoder! resolution: (128, 128)\n",
      "Grid shape: torch.Size([1, 128, 128, 4])\n",
      "Initialzing SlotAttention parameters: T: 3 , K: 7 , D_slots: 128\n",
      "Initialized SlotAttentionDecoder! resolution: (8, 8)\n",
      "Decoder parameters: K: 7 , D_slots: 128\n",
      "Grid shape: torch.Size([1, 8, 8, 4])\n",
      "epoch: 179\n",
      "train_loss: 0.019978486949544468\n",
      "Loaded model!\n"
     ]
    }
   ],
   "source": [
    "model_path = '/kaggle/input/v17-model-180/model_180'\n",
    "print(\"Loading model!\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\" , device)\n",
    "model_dict = torch.load(model_path)\n",
    "model = ObjectDiscovery(encoder_resolution = (128 , 128) , decoder_resolution = (8 , 8),\n",
    "                    T = 3 ,  K = 7 , D_slots = 128)\n",
    "model.to(device)\n",
    "\n",
    "new_dict = {}\n",
    "for key in model_dict['model_state_dict']:\n",
    "    assert key[0 : 7] == 'module.'\n",
    "    new_dict[key[7 : ]] = model_dict['model_state_dict'][key]\n",
    "    \n",
    "model.load_state_dict(new_dict)\n",
    "# model.load_state_dict(model_dict['model_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for key in model_dict:\n",
    "    if key == 'model_state_dict' or key == 'optimizer_state_dict':\n",
    "        continue\n",
    "    print(f'{key}: {model_dict[key]}')\n",
    "\n",
    "print(\"Loaded model!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d7b5c5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T10:10:03.783037Z",
     "iopub.status.busy": "2024-05-07T10:10:03.782414Z",
     "iopub.status.idle": "2024-05-07T10:13:24.440969Z",
     "shell.execute_reply": "2024-05-07T10:13:24.439971Z"
    },
    "papermill": {
     "duration": 200.705796,
     "end_time": "2024-05-07T10:13:24.444413",
     "exception": false,
     "start_time": "2024-05-07T10:10:03.738617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:19<00:00, 50.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI Score: 0.5544671070601245\n",
      "Max ARI Score: 0.9389827038609846\n",
      "Top 10 indices with highest ARI scores: [7661, 2025, 4159, 7213, 9698, 4058, 8231, 8827, 3018, 573, 9489, 8014, 648, 445, 2267, 4153, 5919, 6539, 3270, 6677, 6906, 4925, 9824, 9774, 8713, 2079, 9915, 5896, 8517, 2865, 6129, 7300, 9470, 862, 8046, 3990, 1429, 6010, 8330, 2160, 9400, 4773, 5506, 9588, 9015, 4917, 5071, 9426, 9254, 4793]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import adjusted_rand_score as sk_ari\n",
    "\n",
    "\n",
    "# for CLEVERTex11 data\n",
    "\n",
    "\n",
    "mean_ari = 0 \n",
    "max_ari = 0\n",
    "ari_idx = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in tqdm(range(len(val_data))):\n",
    "        images , true_clusters = val_data[idx]\n",
    "        images , true_clusters = images.to(device) , true_clusters.to(device)\n",
    "#         print(images.shape , true_clusters.shape)\n",
    "        reconstructed , recons , pred_masks  = model(images.unsqueeze(0))\n",
    "        pred_clusters = pred_masks.squeeze(2).argmax(dim = 1).squeeze(0)\n",
    "        pred_clusters , true_clusters = np.array(pred_clusters.cpu()).flatten() , np.array(true_clusters.cpu()).flatten()\n",
    "#         print(\"prec:\" , pred_clusters.shape)\n",
    "#         print(\"true:\" , true_clusters.shape)\n",
    "        ari = sk_ari(labels_true = true_clusters , labels_pred = pred_clusters)\n",
    "        mean_ari += ari\n",
    "        max_ari = max(max_ari , ari)\n",
    "        ari_idx.append((ari , idx))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "print(\"ARI Score:\" , mean_ari / len(val_data))\n",
    "print(\"Max ARI Score:\" , max_ari)\n",
    "ari_idx.sort(reverse = True)\n",
    "top_indices = [index for  _ , index in ari_idx[ : 50]]\n",
    "print(\"Top 10 indices with highest ARI scores:\", top_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87a600f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T10:13:24.815608Z",
     "iopub.status.busy": "2024-05-07T10:13:24.814908Z",
     "iopub.status.idle": "2024-05-07T10:15:24.643795Z",
     "shell.execute_reply": "2024-05-07T10:15:24.642682Z"
    },
    "papermill": {
     "duration": 120.012936,
     "end_time": "2024-05-07T10:15:24.646344",
     "exception": false,
     "start_time": "2024-05-07T10:13:24.633408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices shape: (5319,)\n",
      "clusters data shape: (10000, 128, 128)\n",
      "Loading data!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5319/5319 [00:11<00:00, 466.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data!\n",
      "data size: 5319\n",
      "image tensor max-min 0.8588235378265381 -0.9843137264251709\n",
      "cluster tensor max-min 6 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5319/5319 [01:47<00:00, 49.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI Score: 0.5626948637022806\n",
      "Max ARI Score: 0.9389827038609846\n",
      "Top 10 indices with highest ARI scores: [4092, 1086, 2219, 3852, 5151, 2167, 4380, 4682, 1619, 313, 5051, 4273, 354, 246, 1212, 2214, 3154, 3504, 1751, 3576, 3687, 2630, 5225, 5196, 4622, 1116, 5278, 3143, 4516, 1526, 3277, 3894, 5038, 467, 4291, 2139, 768, 3210, 4428, 1159, 4998, 2554, 2933, 5099, 4779, 2626, 2710, 5010, 4910, 2566]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# for CLEVERTex6 data\n",
    "val_data = ImageDatasetSlotAttention(images_folder = images_val , indices = clever6_indices,\n",
    "                                    transform = transform, cluster_path = val_cluster_path)\n",
    "print(\"image tensor max-min\" , val_data[0][0].max().item() , val_data[0][0].min().item())\n",
    "print(\"cluster tensor max-min\" , val_data[4][1].max().item() , val_data[4][1].min().item())\n",
    "\n",
    "\n",
    "mean_ari = 0 \n",
    "max_ari = 0\n",
    "ari_idx = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in tqdm(range(len(val_data))):\n",
    "        images , true_clusters = val_data[idx]\n",
    "        images , true_clusters = images.to(device) , true_clusters.to(device)\n",
    "#         print(images.shape , true_clusters.shape)\n",
    "        reconstructed , recons , pred_masks  = model(images.unsqueeze(0))\n",
    "        pred_clusters = pred_masks.squeeze(2).argmax(dim = 1).squeeze(0)\n",
    "        pred_clusters , true_clusters = np.array(pred_clusters.cpu()).flatten() , np.array(true_clusters.cpu()).flatten()\n",
    "#         print(\"prec:\" , pred_clusters.shape)\n",
    "#         print(\"true:\" , true_clusters.shape)\n",
    "        ari = sk_ari(labels_true = true_clusters , labels_pred = pred_clusters)\n",
    "        mean_ari += ari\n",
    "        max_ari = max(max_ari , ari)\n",
    "        ari_idx.append((ari , idx))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "print(\"ARI Score:\" , mean_ari / len(val_data))\n",
    "print(\"Max ARI Score:\" , max_ari)\n",
    "ari_idx.sort(reverse = True)\n",
    "top_indices = [index for  _ , index in ari_idx[ : 50]]\n",
    "print(\"Top 10 indices with highest ARI scores:\", top_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7844935c",
   "metadata": {
    "papermill": {
     "duration": 0.27321,
     "end_time": "2024-05-07T10:15:25.198803",
     "exception": false,
     "start_time": "2024-05-07T10:15:24.925593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4942474,
     "sourceId": 8320688,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4948751,
     "sourceId": 8333505,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4956877,
     "sourceId": 8344752,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 395.227625,
   "end_time": "2024-05-07T10:15:27.343387",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-07T10:08:52.115762",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
