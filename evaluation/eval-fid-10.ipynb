{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcba9204",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-07T10:48:20.937476Z",
     "iopub.status.busy": "2024-05-07T10:48:20.937177Z",
     "iopub.status.idle": "2024-05-07T10:48:39.927724Z",
     "shell.execute_reply": "2024-05-07T10:48:39.926582Z"
    },
    "papermill": {
     "duration": 18.999089,
     "end_time": "2024-05-07T10:48:39.930383",
     "exception": false,
     "start_time": "2024-05-07T10:48:20.931294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pykeops\r\n",
      "  Downloading pykeops-2.2.3.tar.gz (92 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting clean-fid\r\n",
      "  Downloading clean_fid-0.1.35-py3-none-any.whl.metadata (36 kB)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pykeops) (1.26.4)\r\n",
      "Requirement already satisfied: pybind11 in /opt/conda/lib/python3.10/site-packages (from pykeops) (2.12.0)\r\n",
      "Collecting keopscore==2.2.3 (from pykeops)\r\n",
      "  Downloading keopscore-2.2.3.tar.gz (100 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clean-fid) (2.1.2)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clean-fid) (0.16.2)\r\n",
      "Requirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from clean-fid) (1.11.4)\r\n",
      "Requirement already satisfied: tqdm>=4.28.1 in /opt/conda/lib/python3.10/site-packages (from clean-fid) (4.66.1)\r\n",
      "Requirement already satisfied: pillow>=8.1 in /opt/conda/lib/python3.10/site-packages (from clean-fid) (9.5.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from clean-fid) (2.31.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->clean-fid) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->clean-fid) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->clean-fid) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->clean-fid) (2024.2.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clean-fid) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->clean-fid) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clean-fid) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clean-fid) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clean-fid) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->clean-fid) (2024.2.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clean-fid) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clean-fid) (1.3.0)\r\n",
      "Downloading clean_fid-0.1.35-py3-none-any.whl (26 kB)\r\n",
      "Building wheels for collected packages: pykeops, keopscore\r\n",
      "  Building wheel for pykeops (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pykeops: filename=pykeops-2.2.3-py3-none-any.whl size=118639 sha256=c1f3c0360cdd898f7e4e048c0f8961b1982a5c6b224dc2b2df25a29e487d3139\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/37/8b/c7/25e5194a7138fd564c3ef3e275ae0155c207cd85d7ab347817\r\n",
      "  Building wheel for keopscore (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for keopscore: filename=keopscore-2.2.3-py3-none-any.whl size=172485 sha256=b9764674625671253ab0cf3350a905b3743ac9744135c9071ee9e8aa156de7f0\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/3d/d8/ee/29900acfbbd7ee0f3b05981d3d172baad4c3b5d40cbf4c5d74\r\n",
      "Successfully built pykeops keopscore\r\n",
      "Installing collected packages: keopscore, pykeops, clean-fid\r\n",
      "Successfully installed clean-fid-0.1.35 keopscore-2.2.3 pykeops-2.2.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pykeops clean-fid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62cdb7ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T10:48:39.946169Z",
     "iopub.status.busy": "2024-05-07T10:48:39.945249Z",
     "iopub.status.idle": "2024-05-07T10:48:58.758770Z",
     "shell.execute_reply": "2024-05-07T10:48:58.757874Z"
    },
    "papermill": {
     "duration": 18.823436,
     "end_time": "2024-05-07T10:48:58.760970",
     "exception": false,
     "start_time": "2024-05-07T10:48:39.937534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's Go!\n",
      "[KeOps] Compiling cuda jit compiler engine ... OK\n",
      "[pyKeOps] Compiling nvrtc binder for python ... OK\n",
      "torch version: 2.1.2\n"
     ]
    }
   ],
   "source": [
    "print(\"Let's Go!\" )\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "import random \n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LayerNorm\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn import BatchNorm2d\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import Linear\n",
    "from torch.nn import DataParallel\n",
    "from torch.nn import Sequential\n",
    "from torch.nn import ConvTranspose2d\n",
    "from torch.nn import GRUCell\n",
    "from torch.nn import Module\n",
    "from torch.nn import Flatten\n",
    "from torch.nn.init import normal_\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import softmax\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Conv1d\n",
    "from torch.nn import MSELoss\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import matplotlib.pyplot as plt\n",
    "from pykeops.torch import LazyTensor\n",
    "from cleanfid import fid\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "print(\"torch version:\" , torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44a8e33a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T10:48:58.775609Z",
     "iopub.status.busy": "2024-05-07T10:48:58.775224Z",
     "iopub.status.idle": "2024-05-07T10:49:48.950136Z",
     "shell.execute_reply": "2024-05-07T10:49:48.949030Z"
    },
    "papermill": {
     "duration": 50.184899,
     "end_time": "2024-05-07T10:49:48.952607",
     "exception": false,
     "start_time": "2024-05-07T10:48:58.767708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices shape: (10000,)\n",
      "Loading data!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:49<00:00, 200.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data!\n",
      "data size: 10000\n",
      "image tensor max-min 0.9686274528503418 -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images_val = '/kaggle/input/clevertexv11-val-images/CLEVERTex10_val_images'\n",
    "# clever6_indices_path = '/kaggle/input/clevertex6-val/CLEVERTex6_val/CLEVRTex6_val_indices.npy'\n",
    "# clever6_indices = np.load(clever6_indices_path)\n",
    "clever10_indices = np.array([40000 + i for i in range(10000)])\n",
    "\n",
    "class ImageDatasetSlotAttention(Dataset):\n",
    "    def __init__(self, images_folder , indices , transform):\n",
    "        super(ImageDatasetSlotAttention , self).__init__()\n",
    "        self.images_folder = images_folder\n",
    "        self.transform = transform\n",
    "        self.indices = indices\n",
    "        print(\"indices shape:\" , self.indices.shape)\n",
    "        self.all_data =  self.build_data(images_folder)\n",
    "        print(\"data size:\" , len(self.all_data))\n",
    "        del self.transform \n",
    "\n",
    "    def build_data(self , folder_path):\n",
    "        files = sorted(os.listdir(folder_path))\n",
    "        files = sorted(files)\n",
    "        data = []\n",
    "        print(\"Loading data!\")\n",
    "        for idx in tqdm(self.indices):\n",
    "            image_path = self.images_folder + f'/CLEVRTEX_full_{idx:06d}.png'\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            data.append(self.transform(image))\n",
    "            \n",
    "        print(\"Loaded data!\")\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self , idx):\n",
    "        return self.all_data[idx]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "  transforms.PILToTensor(),\n",
    "  transforms.ConvertImageDtype(torch.float),\n",
    "  transforms.Normalize((0.5 ,  0.5 , 0.5), (0.5, 0.5 , 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "val_data = ImageDatasetSlotAttention(images_folder = images_val,\n",
    "                                    indices = clever10_indices , transform = transform)\n",
    "print(\"image tensor max-min\" , val_data[0].max().item() , val_data[0].min().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2bb066e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T10:49:49.041352Z",
     "iopub.status.busy": "2024-05-07T10:49:49.041029Z",
     "iopub.status.idle": "2024-05-07T10:49:49.054947Z",
     "shell.execute_reply": "2024-05-07T10:49:49.054111Z"
    },
    "papermill": {
     "duration": 0.059051,
     "end_time": "2024-05-07T10:49:49.056794",
     "exception": false,
     "start_time": "2024-05-07T10:49:48.997743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(Module):\n",
    "    def __init__(self, channels):\n",
    "        super(Block, self).__init__()\n",
    "        self.downsample = Sequential(OrderedDict([\n",
    "            ('conv1' , Conv2d(channels, channels, kernel_size = 5 , stride = 1 , padding = 2)),\n",
    "            ('bn1'   , BatchNorm2d(channels)),\n",
    "            ('relu1' , ReLU()),\n",
    "            ('conv2' , Conv2d(channels, channels, kernel_size = 5 , stride = 1 , padding = 2)),\n",
    "            ('bn2'   , BatchNorm2d(channels))\n",
    "        ]))\n",
    "        \n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + self.downsample(x))\n",
    "\n",
    "class BlockUp(Module):\n",
    "    def __init__(self , in_channels , out_channels):\n",
    "        super(BlockUp, self).__init__()\n",
    "        \n",
    "        self.downsample = Sequential(OrderedDict([\n",
    "            ('conv1' , Conv2d(in_channels , out_channels , kernel_size = 5 , stride = 1 , padding = 2)),\n",
    "            ('bn1'   , BatchNorm2d(out_channels)),\n",
    "            ('relu1' , ReLU()),\n",
    "            ('conv2' , Conv2d(out_channels , out_channels , kernel_size = 5 , stride = 1 , padding = 2)),\n",
    "            ('bn2'   , BatchNorm2d(out_channels)),\n",
    "        ]))\n",
    "        self.skip = Sequential(OrderedDict([\n",
    "            ('conv1' , Conv2d(in_channels , out_channels, kernel_size = 5 , stride = 1 , padding = 2)),\n",
    "            ('bn1'   , BatchNorm2d(out_channels))\n",
    "        ]))\n",
    "        self.relu = ReLU()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.skip(x) + self.downsample(x))\n",
    "    \n",
    "    \n",
    "class ResNet18(Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(ResNet18, self).__init__()\n",
    "        layer0 = Sequential(OrderedDict([\n",
    "         ('conv' , Conv2d(3 , 16 , kernel_size = 5 , stride = 1 , padding = 2)),\n",
    "         ('bn'   , BatchNorm2d(16)),\n",
    "         ('relu' , ReLU())\n",
    "        ]))\n",
    "\n",
    "        self.resnet = Sequential(OrderedDict([\n",
    "            ('layer0' , layer0),\n",
    "            ('block1' , Block(16)),\n",
    "            ('block2' , Block(16)),\n",
    "            ('block3' , BlockUp(16 , 32)),\n",
    "            ('block4' , Block(32)),\n",
    "            ('block5' , Block(32)),\n",
    "            ('block6' , BlockUp(32 , 64)),\n",
    "            ('block7' , Block(64)),\n",
    "            ('block8' , Block(64))\n",
    "        ]))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f201d4dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T10:49:49.141960Z",
     "iopub.status.busy": "2024-05-07T10:49:49.141683Z",
     "iopub.status.idle": "2024-05-07T10:49:49.175222Z",
     "shell.execute_reply": "2024-05-07T10:49:49.174385Z"
    },
    "papermill": {
     "duration": 0.078635,
     "end_time": "2024-05-07T10:49:49.177108",
     "exception": false,
     "start_time": "2024-05-07T10:49:49.098473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ObjectDiscovery(Module):\n",
    "    def __init__(self , encoder_resolution ,  decoder_resolution , T   , K , D_slots):\n",
    "        super(ObjectDiscovery , self).__init__()\n",
    "        print(f'Initialized ObjectDiscovery!')\n",
    "        self.layers = Sequential(OrderedDict([\n",
    "          (\"encoder\" , ImageEncoder(resolution = encoder_resolution , T = T,\n",
    "                                    K = K , D_slots = D_slots )),\n",
    "          (\"decoder\" , SlotAttentionDecoder(resolution = decoder_resolution , K = K , D_slots = D_slots))\n",
    "        ]))\n",
    "    def forward(self , image):\n",
    "        return self.layers(image)\n",
    "\n",
    "class PositionEncoder(Module):\n",
    "    def __init__(self, output_dim , resolution):\n",
    "        super(PositionEncoder , self).__init__()\n",
    "        self.linear =  Linear(in_features = 4 , out_features = output_dim)\n",
    "        # above is equivalent to a linear layer\n",
    "        self.grid = Parameter(data = PositionEncoder.build_grid(resolution) , requires_grad = False)\n",
    "        print(\"Grid shape:\" , self.grid.shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_grid(resolution):\n",
    "        ranges = [np.linspace(start = 0.0 , stop = 1.0 , num = dimension) for dimension in resolution] # dim = (2 , 128)\n",
    "        grid = np.meshgrid(*ranges , sparse = False, indexing = \"ij\") # dim = (128 , 128)\n",
    "        # row[i] of grid[0] has all elements i / 127 and col[j] of grid[1] has all elements j / 127\n",
    "        grid = np.stack(grid , axis = -1) # dim = (64 , 64 , 2) to match conv later\n",
    "        grid = np.expand_dims(grid, axis = 0) # dim = (1 , 64 , 64 , 2) for batch dimension\n",
    "        grid = grid.astype(np.float32) # PyTorch throws an error later otherwise\n",
    "        return torch.tensor(np.concatenate([grid , 1.0 - grid] , axis = 3)) # (1 , 64 , 64 , 4)\n",
    "\n",
    "\n",
    "    def forward(self, x): # x has shape (batch , 64 , 64 , D_inputs)\n",
    "        return x + self.linear(self.grid)\n",
    "\n",
    "\n",
    "class ImageEncoder(Module):\n",
    "    def __init__(self , resolution , T  ,  K , D_slots):\n",
    "        super(ImageEncoder , self).__init__()\n",
    "        print(f'Initialized ImageEncoder! resolution: {resolution}')\n",
    "        D_inputs = 64\n",
    "        self.encoder_cnn = ResNet18()\n",
    "        down_resolution = (128 , 128)\n",
    "        positional_encoder = PositionEncoder(output_dim =  D_inputs , resolution = down_resolution)\n",
    "\n",
    "\n",
    "        slot_attention = SlotAttention(T = T , K = K , D_slots = D_slots)\n",
    "        self.pos_encode_feedforward_slotattn = Sequential(OrderedDict([\n",
    "            (\"pos_encoder\" , positional_encoder), #  (batch , 64 , 64 , D_inputs)\n",
    "            (\"flatten\" , Flatten(start_dim = 1 , end_dim = 2)), # (batch , 64 * 64 , D_inputs)\n",
    "            (\"layer_norm\" ,  LayerNorm(normalized_shape = 64)), # (batch , 64 * 64 , D_inputs)\n",
    "            (\"linear1:\" , Linear(in_features = 64 , out_features = 128)),\n",
    "            (\"relu1:\"   , ReLU()),\n",
    "            (\"linear2:\" , Linear(in_features = 128 , out_features = 128)),\n",
    "            (\"slot_attention\" , slot_attention) #  (batch , K , D_slots)\n",
    "        ]))\n",
    "    # feature map has shape (channels , h , w)\n",
    "    # N is h * w i.e. each pixel is a different feature \"vector\", size of this vector of D_inputs = # of channels\n",
    "    # channels = D_inputs , h * w = N\n",
    "    def forward(self , x): # x is aimge with shape (batch , 3 , 126 , 128)\n",
    "        x = self.encoder_cnn(x).permute(dims = (0 , 2 , 3 , 1))  # (batch , 64 , 64 , D_inputs)\n",
    "        return self.pos_encode_feedforward_slotattn(x)\n",
    "\n",
    "\n",
    "\n",
    "class SlotAttention(Module):\n",
    "    def __init__(self , T, K , D_slots  , epsilon = 1e-8):\n",
    "        super(SlotAttention , self).__init__()\n",
    "        print(f'Initialzing SlotAttention parameters: T: {T} , K: {K} , D_slots: {D_slots}')\n",
    "        self.T = T\n",
    "        self.K = K\n",
    "        self.D_slots = D_slots\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.norm_inputs = LayerNorm(normalized_shape =  128)\n",
    "        self.query_from_slots  = Sequential(OrderedDict([\n",
    "            (\"NormSlots\" , LayerNorm(normalized_shape = D_slots)),\n",
    "            (\"SlotQuery\" , Linear(in_features = D_slots  , out_features = D_slots , bias = False))\n",
    "        ]))\n",
    "\n",
    "\n",
    "#         self.slots_init = Parameter(data = normal_(torch.empty(1 , K , D_slots)) , requires_grad = True)\n",
    "\n",
    "        self.init_latents = Parameter(normal_(torch.empty(1 , self.K , self.D_slots)))\n",
    "\n",
    "\n",
    "\n",
    "        self.keys_from_inputs  = Linear(in_features = 128 , out_features = D_slots , bias = False)\n",
    "        self.vals_from_inputs  = Linear(in_features = 128 , out_features = D_slots , bias = False)\n",
    "        self.gru = GRUCell(input_size = D_slots , hidden_size = D_slots)\n",
    "        self.norm_feed = Sequential(OrderedDict([\n",
    "            (\"layer_norm\" ,  LayerNorm(normalized_shape = D_slots)),\n",
    "            (\"linear1\" , Linear(in_features = D_slots , out_features = 256)),\n",
    "            (\"relu1\"   , ReLU()),\n",
    "            (\"linear2\" , Linear(in_features = 256 , out_features = D_slots))\n",
    "        ]))\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.shape[0]\n",
    "        inputs = self.norm_inputs(inputs)\n",
    "        keys = self.keys_from_inputs(inputs)\n",
    "        vals = self.vals_from_inputs(inputs)\n",
    "        slots = self.init_latents.repeat(batch_size , 1 , 1)\n",
    "        for _ in range(self.T):\n",
    "            slots_prev = slots\n",
    "            queries = self.query_from_slots(slots)\n",
    "            attn_logits = (self.D_slots ** -0.5) * torch.einsum('bnc,bmc->bnm', keys , queries)\n",
    "            attn = F.softmax(attn_logits , dim = -1)\n",
    "            attn = attn + self.epsilon\n",
    "            attn = attn / torch.sum(attn, dim = 1, keepdim = True)\n",
    "            updates = torch.einsum('bnm,bnc->bmc' , attn , vals)\n",
    "            slots = self.gru(\n",
    "                updates.view(batch_size * self.K , self.D_slots),\n",
    "                slots_prev.view(batch_size * self.K , self.D_slots),\n",
    "            )\n",
    "            slots = slots.view(batch_size , self.K , self.D_slots)\n",
    "            slots = slots + self.norm_feed(slots)\n",
    "\n",
    "        return slots\n",
    "\n",
    "\n",
    "class SlotAttentionDecoder(Module):\n",
    "    def __init__(self , resolution , K , D_slots):\n",
    "        super(SlotAttentionDecoder , self).__init__()\n",
    "        print(f'Initialized SlotAttentionDecoder! resolution: {resolution}')\n",
    "        print(f'Decoder parameters: K: {K} , D_slots: {D_slots}')\n",
    "        self.resolution = resolution\n",
    "        self.positional_encoder = PositionEncoder(output_dim = D_slots , resolution = resolution)\n",
    "\n",
    "        self.K = K\n",
    "        self.D_slots = D_slots\n",
    "\n",
    "        self.decoder_cnn = Sequential(OrderedDict([\n",
    "            (\"conv1\" , ConvTranspose2d(in_channels = D_slots , out_channels = 64 , kernel_size = 5 , stride = 2 , padding = 2 , output_padding = 1)),\n",
    "            ('relu1' , ReLU()),\n",
    "            (\"conv2\" , ConvTranspose2d(in_channels = 64 , out_channels = 64 , kernel_size = 5 , stride = 2 , padding = 2 , output_padding = 1)),\n",
    "            ('relu2' , ReLU()),\n",
    "            (\"conv3\" , ConvTranspose2d(in_channels = 64 , out_channels = 64 , kernel_size = 5 , stride = 2 , padding = 2 , output_padding = 1)),\n",
    "            ('relu3' , ReLU()),\n",
    "            (\"conv4\" , ConvTranspose2d(in_channels = 64 , out_channels = 64 , kernel_size = 5 , stride = 2 , padding = 2 , output_padding = 1)),\n",
    "            ('relu4' , ReLU()),\n",
    "            (\"conv5\" , ConvTranspose2d(in_channels = 64 , out_channels = 4 , kernel_size = 1 , stride = 1)),\n",
    "        ]))\n",
    "\n",
    "\n",
    "    def forward(self , slots):\n",
    "        # slots comes form the ImageEncoder shape : (batch , K , D_slots)\n",
    "        batch_size = slots.shape[0]\n",
    "        x =  slots.view(batch_size * self.K , 1 , 1 , self.D_slots)\n",
    "        x = x.repeat(1 , self.resolution[0] , self.resolution[1] , 1)\n",
    "        x = self.positional_encoder(x).permute(0 , 3 , 1 , 2)\n",
    "        x = self.decoder_cnn(x) # dim = (batch * K , 4 , 128 , 128)\n",
    "        x = x.view(batch_size, self.K , 4 , 128 , 128)# dim = (batch , K , 4 , 128 , 128)\n",
    "        recons , masks = x[: , : , : 3 , : , :]  , x[: , : , -1 : , : , :]\n",
    "        masks = F.softmax(masks , dim = 1) # dim = (batch , K , 1 , 128, 128)\n",
    "        reconstructed = torch.sum(recons * masks , dim = 1) # dim = (batch , 3 , 128, 128)\n",
    "        return reconstructed , recons , masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cf5359b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T10:49:49.262784Z",
     "iopub.status.busy": "2024-05-07T10:49:49.262499Z",
     "iopub.status.idle": "2024-05-07T10:49:49.908967Z",
     "shell.execute_reply": "2024-05-07T10:49:49.907944Z"
    },
    "papermill": {
     "duration": 0.691448,
     "end_time": "2024-05-07T10:49:49.911099",
     "exception": false,
     "start_time": "2024-05-07T10:49:49.219651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model!\n",
      "Using device: cuda\n",
      "Initialized ObjectDiscovery!\n",
      "Initialized ImageEncoder! resolution: (128, 128)\n",
      "Grid shape: torch.Size([1, 128, 128, 4])\n",
      "Initialzing SlotAttention parameters: T: 3 , K: 7 , D_slots: 128\n",
      "Initialized SlotAttentionDecoder! resolution: (8, 8)\n",
      "Decoder parameters: K: 7 , D_slots: 128\n",
      "Grid shape: torch.Size([1, 8, 8, 4])\n",
      "epoch: 179\n",
      "train_loss: 0.019978486949544468\n",
      "Loaded model!\n"
     ]
    }
   ],
   "source": [
    "model_path = '/kaggle/input/v17-model-180/model_180'\n",
    "print(\"Loading model!\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\" , device)\n",
    "model_dict = torch.load(model_path)\n",
    "model = ObjectDiscovery(encoder_resolution = (128 , 128) , decoder_resolution = (8 , 8),\n",
    "                    T = 3 ,  K = 7 , D_slots = 128)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "new_dict = {}\n",
    "for key in model_dict['model_state_dict']:\n",
    "    assert key[0 : 7] == 'module.'\n",
    "    new_dict[key[7 : ]] = model_dict['model_state_dict'][key]\n",
    "    \n",
    "model.load_state_dict(new_dict)\n",
    "# model.load_state_dict(model_dict['model_state_dict'])\n",
    "encoder_model = model.layers.encoder\n",
    "decoder_model = model.layers.decoder\n",
    "encoder_model.eval()\n",
    "decoder_model.eval()\n",
    "for key in model_dict:\n",
    "    if key == 'model_state_dict' or key == 'optimizer_state_dict':\n",
    "        continue\n",
    "    print(f'{key}: {model_dict[key]}')\n",
    "\n",
    "print(\"Loaded model!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e48c1122",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T10:49:49.997564Z",
     "iopub.status.busy": "2024-05-07T10:49:49.996924Z",
     "iopub.status.idle": "2024-05-07T10:49:50.003314Z",
     "shell.execute_reply": "2024-05-07T10:49:50.002444Z"
    },
    "papermill": {
     "duration": 0.051644,
     "end_time": "2024-05-07T10:49:50.005465",
     "exception": false,
     "start_time": "2024-05-07T10:49:49.953821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: torch.Size([3, 128, 128])\n",
      "val data size  : 10000\n",
      "# val batches  : 157\n",
      "mini-batch size  : 64\n",
      "num_workers val: 1\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "val_loader = DataLoader(val_data , batch_size = BATCH_SIZE , shuffle = True , num_workers = 1 , pin_memory = True)\n",
    "print(\"image shape:\" , val_data[0].shape)\n",
    "print(\"val data size  :\" , len(val_data))\n",
    "print(\"# val batches  :\" , len(val_loader))\n",
    "print(\"mini-batch size  :\" , BATCH_SIZE)\n",
    "print(\"num_workers val:\" , val_loader.num_workers)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2911e15a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T10:49:50.091079Z",
     "iopub.status.busy": "2024-05-07T10:49:50.090821Z",
     "iopub.status.idle": "2024-05-07T10:50:54.548103Z",
     "shell.execute_reply": "2024-05-07T10:50:54.546988Z"
    },
    "papermill": {
     "duration": 64.502676,
     "end_time": "2024-05-07T10:50:54.550312",
     "exception": false,
     "start_time": "2024-05-07T10:49:50.047636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [01:04<00:00,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all slots: torch.Size([70000, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# data_tensor \n",
    "all_slots = torch.zeros((0 , 128)).to(device)\n",
    "with torch.no_grad():\n",
    "    for images in tqdm(val_loader):\n",
    "        images = images.to(device)\n",
    "        slots = encoder_model(images).flatten(start_dim = 0 , end_dim = 1)\n",
    "        all_slots = torch.concat((all_slots , slots) , dim = 0)\n",
    "\n",
    "print(\"all slots:\" , all_slots.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcf0ae9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T10:50:54.661836Z",
     "iopub.status.busy": "2024-05-07T10:50:54.661165Z",
     "iopub.status.idle": "2024-05-07T10:50:57.035386Z",
     "shell.execute_reply": "2024-05-07T10:50:57.034441Z"
    },
    "papermill": {
     "duration": 2.431809,
     "end_time": "2024-05-07T10:50:57.037450",
     "exception": false,
     "start_time": "2024-05-07T10:50:54.605641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 70000 D: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KeOps] Generating code for ArgMin_Reduction reduction (with parameters 0) of formula Sum((a-b)**2) with a=Var(0,128,0), b=Var(1,128,1) ... OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 423.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class indices shape: torch.Size([70000])\n",
      "centroid shape: torch.Size([7, 128])\n",
      "K means clustering finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def KMeans(x , K = 7 , ITRS = 1000):\n",
    "\n",
    "    N , D = x.shape \n",
    "    print(\"N:\" , N , \"D:\" , D)\n",
    "    c = x[: K , :].clone() \n",
    "\n",
    "    x_i = LazyTensor(x.view(N , 1 , D))\n",
    "    c_j = LazyTensor(c.view(1 , K , D))\n",
    "    for i in tqdm(range(ITRS)):\n",
    "        D_ij = ((x_i - c_j) ** 2).sum(-1)\n",
    "        cl = D_ij.argmin(dim=1).long().view(-1)\n",
    "        c.zero_()\n",
    "        c.scatter_add_(0, cl[:, None].repeat(1, D), x)\n",
    "        Ncl = torch.bincount(cl, minlength=K).type_as(c).view(K, 1)\n",
    "        c /= Ncl\n",
    "\n",
    "    print(\"class indices shape:\" , cl.shape)\n",
    "    print(\"centroid shape:\" , c.shape)\n",
    "    print(\"K means clustering finished!\")\n",
    "\n",
    "    return cl\n",
    "\n",
    "cl = KMeans(all_slots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8e9b24d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T10:50:57.195111Z",
     "iopub.status.busy": "2024-05-07T10:50:57.194234Z",
     "iopub.status.idle": "2024-05-07T10:50:58.043834Z",
     "shell.execute_reply": "2024-05-07T10:50:58.042946Z"
    },
    "papermill": {
     "duration": 0.9521,
     "end_time": "2024-05-07T10:50:58.046096",
     "exception": false,
     "start_time": "2024-05-07T10:50:57.093996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster shape: torch.Size([162, 128])\n",
      "cluster shape: torch.Size([10096, 128])\n",
      "cluster shape: torch.Size([14805, 128])\n",
      "cluster shape: torch.Size([9132, 128])\n",
      "cluster shape: torch.Size([10000, 128])\n",
      "cluster shape: torch.Size([11524, 128])\n",
      "cluster shape: torch.Size([14281, 128])\n",
      "Verified clustering size!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 13018.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated data size: 10000\n",
      "generated slot shape: torch.Size([7, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class_tensors = []\n",
    "K = 7\n",
    "total_size = 0\n",
    "for cluster_id in range(K): \n",
    "    mask = (cl == cluster_id)    \n",
    "    filtered_data = all_slots[mask]    \n",
    "    class_tensors.append(filtered_data)\n",
    "    total_size += class_tensors[cluster_id].shape[0]\n",
    "    print(\"cluster shape:\" , filtered_data.shape)\n",
    "assert total_size == all_slots.shape[0]\n",
    "print(\"Verified clustering size!\")\n",
    "all_data = []\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "for idx in tqdm(range(len(val_data))):\n",
    "    slots = []\n",
    "    for slot_id in range(K):\n",
    "        num_examples = class_tensors[slot_id].shape[0]\n",
    "        sample_id = random.randint(0 , num_examples - 1)\n",
    "        slots.append(class_tensors[slot_id][sample_id].unsqueeze(0))\n",
    "    slots = torch.cat(slots , dim = 0)\n",
    "    all_data.append(slots)\n",
    "        \n",
    "print(\"generated data size:\" , len(all_data))\n",
    "print(\"generated slot shape:\" , all_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a9acbc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T10:50:58.162265Z",
     "iopub.status.busy": "2024-05-07T10:50:58.161960Z",
     "iopub.status.idle": "2024-05-07T10:52:42.873131Z",
     "shell.execute_reply": "2024-05-07T10:52:42.872062Z"
    },
    "papermill": {
     "duration": 104.771164,
     "end_time": "2024-05-07T10:52:42.875197",
     "exception": false,
     "start_time": "2024-05-07T10:50:58.104033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory 'generated_images' does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:44<00:00, 95.51it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'generated_images'\n",
    "\n",
    "if os.path.exists(save_dir):\n",
    "    shutil.rmtree(save_dir)\n",
    "    print(f\"The directory '{save_dir}' and its contents have been deleted.\")\n",
    "else:\n",
    "    print(f\"The directory '{save_dir}' does not exist.\")\n",
    "    \n",
    "os.mkdir(save_dir)\n",
    "\n",
    "def show_image(image , file):\n",
    "    pil = Image.fromarray(image)\n",
    "    pil.save(file + '.png')\n",
    "\n",
    "for idx in tqdm(range(len(all_data))):\n",
    "    with torch.no_grad():\n",
    "        image = decoder_model(all_data[idx].unsqueeze(0))[0].squeeze(0)\n",
    "    image  = (255 * (0.5 * image.cpu().numpy().transpose(1, 2, 0) + 0.5)).astype(np.uint8)\n",
    "    show_image(image , file = f'{save_dir}/cluster_mask_{idx}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27f6449d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T10:52:43.142993Z",
     "iopub.status.busy": "2024-05-07T10:52:43.142669Z",
     "iopub.status.idle": "2024-05-07T10:54:59.601962Z",
     "shell.execute_reply": "2024-05-07T10:54:59.600832Z"
    },
    "papermill": {
     "duration": 136.59584,
     "end_time": "2024-05-07T10:54:59.604983",
     "exception": false,
     "start_time": "2024-05-07T10:52:43.009143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute FID between two folders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images in the folder /kaggle/input/clevertexv11-val-images/CLEVERTex10_val_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID CLEVERTex10_val_images : 100%|██████████| 313/313 [01:05<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images in the folder generated_images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID generated_images : 100%|██████████| 313/313 [01:01<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID Score: 161.3660828989385\n"
     ]
    }
   ],
   "source": [
    "score = fid.compute_fid('/kaggle/input/clevertexv11-val-images/CLEVERTex10_val_images' ,\n",
    "                        save_dir)\n",
    "print(\"FID Score:\" , score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48bd8f1",
   "metadata": {
    "papermill": {
     "duration": 0.181537,
     "end_time": "2024-05-07T10:55:00.005824",
     "exception": false,
     "start_time": "2024-05-07T10:54:59.824287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4948751,
     "sourceId": 8333505,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4956877,
     "sourceId": 8344752,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 404.452884,
   "end_time": "2024-05-07T10:55:02.526205",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-07T10:48:18.073321",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
